{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a423bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d.ml.torch as ml3d\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06544bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ml3d.datasets.ShapeNet(dataset_path='datasets/shapeNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split = dataset.get_split('all')\n",
    "\n",
    "# print the attributes of the first datum\n",
    "print(all_split.get_attr(0))\n",
    "\n",
    "# print the shape of the first point cloud\n",
    "print(all_split.get_data(0)['point'].shape)\n",
    "\n",
    "# show the first 100 frames using the visualizer\n",
    "vis = ml3d.vis.Visualizer()\n",
    "vis.visualize_dataset(dataset, 'all', indices=range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62715849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(tag, t):\n",
    "    print(\"{}: {}s\".format(tag, time() - t))\n",
    "    return time()\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstractionMsg(nn.Module):\n",
    "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
    "        super(PointNetSetAbstractionMsg, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius_list = radius_list\n",
    "        self.nsample_list = nsample_list\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        self.bn_blocks = nn.ModuleList()\n",
    "        for i in range(len(mlp_list)):\n",
    "            convs = nn.ModuleList()\n",
    "            bns = nn.ModuleList()\n",
    "            last_channel = in_channel + 3\n",
    "            for out_channel in mlp_list[i]:\n",
    "                convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "                bns.append(nn.BatchNorm2d(out_channel))\n",
    "                last_channel = out_channel\n",
    "            self.conv_blocks.append(convs)\n",
    "            self.bn_blocks.append(bns)\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        B, N, C = xyz.shape\n",
    "        S = self.npoint\n",
    "        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n",
    "        new_points_list = []\n",
    "        for i, radius in enumerate(self.radius_list):\n",
    "            K = self.nsample_list[i]\n",
    "            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n",
    "            grouped_xyz = index_points(xyz, group_idx)\n",
    "            grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, group_idx)\n",
    "                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n",
    "            else:\n",
    "                grouped_points = grouped_xyz\n",
    "\n",
    "            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n",
    "            for j in range(len(self.conv_blocks[i])):\n",
    "                conv = self.conv_blocks[i][j]\n",
    "                bn = self.bn_blocks[i][j]\n",
    "                grouped_points =  F.relu(bn(conv(grouped_points)))\n",
    "            new_points = torch.max(grouped_points, 2)[0]  # [B, D', S]\n",
    "            new_points_list.append(new_points)\n",
    "\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        new_points_concat = torch.cat(new_points_list, dim=1)\n",
    "        return new_xyz, new_points_concat\n",
    "\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)))\n",
    "        return new_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6dedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_model(nn.Module):\n",
    "    def __init__(self, num_classes, normal_channel=False):\n",
    "        super(get_model, self).__init__()\n",
    "        if normal_channel:\n",
    "            additional_channel = 3\n",
    "        else:\n",
    "            additional_channel = 0\n",
    "        self.normal_channel = normal_channel\n",
    "        self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [32, 64, 128], 3+additional_channel, [[32, 32, 64], [64, 64, 128], [64, 96, 128]])\n",
    "        self.sa2 = PointNetSetAbstractionMsg(128, [0.4,0.8], [64, 128], 128+128+64, [[128, 128, 256], [128, 196, 256]])\n",
    "        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=512 + 3, mlp=[256, 512, 1024], group_all=True)\n",
    "        self.fp3 = PointNetFeaturePropagation(in_channel=1536, mlp=[256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(in_channel=576, mlp=[256, 128])\n",
    "        self.fp1 = PointNetFeaturePropagation(in_channel=150+additional_channel, mlp=[128, 128])\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, xyz, cls_label):\n",
    "        # Set Abstraction layers\n",
    "        B,C,N = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            l0_points = xyz\n",
    "            l0_xyz = xyz[:,:3,:]\n",
    "        else:\n",
    "            l0_points = xyz\n",
    "            l0_xyz = xyz\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        # Feature Propagation layers\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "        cls_label_one_hot = cls_label.view(B,16,1).repeat(1,1,N)\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, torch.cat([cls_label_one_hot,l0_xyz,l0_points],1), l1_points)\n",
    "        # FC layers\n",
    "        feat = F.relu(self.bn1(self.conv1(l0_points)))\n",
    "        x = self.drop1(feat)\n",
    "        x = self.conv2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x, l3_points\n",
    "\n",
    "\n",
    "class get_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(get_loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, trans_feat):\n",
    "        total_loss = F.nll_loss(pred, target)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18af5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pts_file(pts_path):\n",
    "    try:\n",
    "        data = np.loadtxt(pts_path, delimiter=' ', dtype=np.float32)\n",
    "        points = data[:, :3] \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении PTS-файла: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return points\n",
    "\n",
    "def preprocess_point_cloud(points, num_points=2048, compute_normals=True): \n",
    "    \"\"\"\n",
    "    Центрирование, нормализация, сэмплирование и добавление нормалей\n",
    "    \"\"\"\n",
    "    if points.shape[0] < num_points:\n",
    "        raise ValueError(f\"Недостаточно точек ({points.shape[0]}) для требуемого числа ({num_points}).\")\n",
    "    \n",
    "    if points.shape[0] > num_points:\n",
    "        choice = np.random.choice(points.shape[0], num_points, replace=False)\n",
    "        points = points[choice, :]\n",
    "    \n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    m = np.max(np.sqrt(np.sum(points_centered**2, axis=1)))\n",
    "    points_normalized = points_centered / m\n",
    "    \n",
    "    combined_features = points_normalized\n",
    "    \n",
    "    if compute_normals:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points_normalized)\n",
    "        \n",
    "        pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=30))\n",
    "        points_normals = np.asarray(pcd.normals).astype(np.float32)\n",
    "        \n",
    "        combined_features = np.concatenate([points_normalized, points_normals], axis=1) # [N, 6]\n",
    "        \n",
    "    points_tensor = torch.from_numpy(combined_features).unsqueeze(0).transpose(2, 1)\n",
    "    \n",
    "    return points_tensor, combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54e781d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Airplane',\n",
       " 1: 'Bag',\n",
       " 2: 'Cap',\n",
       " 3: 'Car',\n",
       " 4: 'Chair',\n",
       " 5: 'Earphone',\n",
       " 6: 'Guitar',\n",
       " 7: 'Knife',\n",
       " 8: 'Lamp',\n",
       " 9: 'Laptop',\n",
       " 10: 'Motorbike',\n",
       " 11: 'Mug',\n",
       " 12: 'Pistol',\n",
       " 13: 'Rocket',\n",
       " 14: 'Skateboard',\n",
       " 15: 'Table'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = dataset.get_label_to_names()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfb8cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса успешно загружены из: models/part_seg.pth\n"
     ]
    }
   ],
   "source": [
    "PTS_FILE_PATH = 'datasets/shapeNet/shapenetcore_partanno_segmentation_benchmark_v0/03948459/points/2acea60043da129a5a003776cb72cb3a.pts'\n",
    "original_points_coords = load_pts_file(PTS_FILE_PATH)\n",
    "CATEGORY_ID = '03948459' \n",
    "NUM_CLASSES_PART = 50 \n",
    "\n",
    "CAT_TO_IDX = {\n",
    "    '02691185': 0, # airplane\n",
    "    '02773838': 1, # bag\n",
    "    '02954340': 2, # cap\n",
    "    '02958343': 3, # car\n",
    "    '03001627': 4, # chair\n",
    "    '03261776': 5, # earphone\n",
    "    '03467517': 6, # guitar\n",
    "    '03624134': 7, # knife\n",
    "    '03636649': 8, # lamp\n",
    "    '03642806': 9, # laptop\n",
    "    '03790512': 10, # motorbike\n",
    "    '03797390': 11, # mug\n",
    "    '03948459': 12, # pistol\n",
    "    '04099429': 13, # rocket\n",
    "    '04225987': 14, # skateboard\n",
    "    '04379243': 15, # table\n",
    "}\n",
    "\n",
    "points_tensor, normalized_points_coords = preprocess_point_cloud(original_points_coords)\n",
    "\n",
    "WEIGHTS_PATH = \"models/part_seg.pth\" \n",
    "\n",
    "model = get_model(num_classes=NUM_CLASSES_PART, normal_channel=True)\n",
    "model = model.eval()\n",
    "try:\n",
    "    checkpoint = torch.load(WEIGHTS_PATH, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Веса успешно загружены из: {WEIGHTS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки весов: {e}\")\n",
    "    exit()\n",
    "\n",
    "points_tensor, normalized_points_coords = preprocess_point_cloud(original_points_coords, compute_normals=True)\n",
    "NUM_TOTAL_CATEGORIES = 16\n",
    "cat_index = CAT_TO_IDX.get(CATEGORY_ID)\n",
    "cls_index_tensor = torch.tensor([cat_index], dtype=torch.long)\n",
    "cls_label = F.one_hot(cls_index_tensor, num_classes=NUM_TOTAL_CATEGORIES).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_logits, _ = model(points_tensor, cls_label)\n",
    "    \n",
    "    seg_pred = seg_logits.max(dim=2)[1] \n",
    "    \n",
    "    segment_ids = seg_pred.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28af5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_segments = np.unique(segment_ids)\n",
    "num_unique = len(unique_segments)\n",
    "\n",
    "id_map = {seg_id: i for i, seg_id in enumerate(unique_segments)}\n",
    "\n",
    "mapped_ids = np.array([id_map[id] for id in segment_ids])\n",
    "\n",
    "cmap = plt.get_cmap(\"jet\") \n",
    "colors_rgba = cmap(mapped_ids / (num_unique - 1 + 1e-6)) \n",
    "colors_rgb = colors_rgba[:, :3]\n",
    "\n",
    "\n",
    "pcd_result = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd_result.points = o3d.utility.Vector3dVector(normalized_points_coords[:, :3]) \n",
    "\n",
    "pcd_result.colors = o3d.utility.Vector3dVector(colors_rgb) \n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_result], \n",
    "                                  window_name=\"ShapeNet Part Segmentation Result\",\n",
    "                                  point_show_normal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a69a639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказаны уникальные сегменты: [12 13 14]\n",
      "Количество уникальных сегментов: 3\n"
     ]
    }
   ],
   "source": [
    "unique_segments = np.unique(segment_ids)\n",
    "print(f\"Предсказаны уникальные сегменты: {unique_segments}\")\n",
    "print(f\"Количество уникальных сегментов: {len(unique_segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1f323",
   "metadata": {},
   "source": [
    "Result: chair_1\n",
    "\n",
    "![](src/result_segmentation/res_chair_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506aaf6",
   "metadata": {},
   "source": [
    "Result chair_2:\n",
    "\n",
    "![](src/result_segmentation/res_chair_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718986b1",
   "metadata": {},
   "source": [
    "Result knife:\n",
    "\n",
    "![](src/result_segmentation/res_knife.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6b316",
   "metadata": {},
   "source": [
    "Result pistol:\n",
    "\n",
    "![](src/result_segmentation/res_pistol.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
