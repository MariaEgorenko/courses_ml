import pandas as pd
import numpy as np
from copy import deepcopy


class DecisionTreeCARD:

    def __init__(self, max_depth=100, min_samples=2, ccp_alpha=0.0, regression=False):
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.ccp_alpha = ccp_alpha
        self.regression = regression
        self.tree = None
        self._y_type = None
        self._num_all_samples = None

    def _set_df_types(self, X, y, dtype):
        X = X.astype(dtype)
        y = y.astype(dtype) if self.regression else y
        self._y_dtype = y.dtype

        return X, y
    
    @staticmethod
    def _purity(y):
        unique_classes = np.unique(y)

        return unique_classes.size == 1
    
    @staticmethod
    def _is_leaf_node(node):
        return not isinstance(node, dict)  # если узел является листом
    
    def _leaf_node(self, y):
        class_index = 0

        return np.mean(y) if self.regression else y.mode()[class_index]
    
    def _split_df(self, X, y, feature, threshold):
        feature_values = X[feature]
        left_indexes = X[feature_values <= threshold].index
        right_indexes = X[feature_values > threshold].index
        sizes = np.array([left_indexes.size, right_indexes.size])

        return self._leaf_node(y) if any(sizes == 0) else left_indexes, right_indexes
    
    @staticmethod
    def _gini_impuriry(y):
        _, counts_classes = np.unique(y, return_counts=True)
        squared_probabilities = np.square(counts_classes / y.size)
        gini_impurity = 1 - sum(squared_probabilities)

        return gini_impurity
    
    @staticmethod
    def _mse(y):
        mse = np.mean((y - y.mean()) ** 2)

        return mse
    
    @staticmethod
    def _cost_function(left_df, right_df, method):
        total_df_size = left_df.size + right_df.size
        p_left_df = left_df.size / total_df_size
        p_right_df = right_df.size / total_df_size
        J_left = method(left_df)
        J_right = method(right_df)
        J = p_left_df*J_left + p_right_df*J_right

        return J
    
    def _node_error_rate(self, y, method):
        if self._num_all_samples is None:
            self._num_all_samples = y.size
        current_num_samples = y.size

        return current_num_samples / self._num_all_samples * method(y)
    
    def _best_split(self, X, y):
        features = X.columns
        min_count_function = np.inf
        best_features, best_threshold = None, None
        method = self._mse if self.regression else self._gini_impuriry

        for feature in features:
            unique_feature_values = np.unique(X[feature])

            for i in range(1, len(unique_feature_values)):
                current_value = unique_feature_values[i]
                previos_value = unique_feature_values[i - 1]
                threshold = (current_value + previos_value) / 2
                left_indexes, right_indexes = self._split_df(X, y, feature, threshold)
                left_labels, right_labels = y.loc[left_indexes], y.loc[right_indexes]
                current_J = self._cost_function(left_labels, right_labels, method)

                if current_J <= min_count_function:
                    min_count_function = current_J
                    best_features = feature
                    best_threshold = threshold

        return best_features, best_threshold
    
    def _stopping_conditions(self, y, depth, n_samples):
        return self._purity(y), depth == self.max_depth, n_samples < self.min_samples

    def _grow_tree(self, X, y, depth=0):
        current_num_samples = y.size
        X, y = self._set_df_types(X, y, np.float128)
        method = self._mse if self.regression else self._gini_impuriry

        if any(self._stopping_conditions(y, depth, current_num_samples)):
            RTi = self._node_error_rate(y, method)
            leaf_node = f'{self._leaf_node(y)} | error_rate {RTi}'
            return leaf_node
        
        Rt = self._node_error_rate(y, method)
        best_feature, best_threshold = self._best_split(X, y)
        decision_node = f'{best_feature} <= {best_threshold} | ' \
                        f'as_leaf {self._leaf_node(y)} error_rate {Rt}'
        
        left_indexes, right_indexes = self._split_df(X, y, best_feature, best_threshold)
        left_X, right_X = X.loc[left_indexes], X.loc[right_indexes]
        left_labels, right_labels = y.loc[left_indexes], y.loc[right_indexes]

        tree = {decision_node: []}
        left_subtree = self._grow_tree(left_X, left_labels, depth + 1)
        right_subtree = self._grow_tree(right_X, right_labels, depth + 1)

        if left_subtree == right_subtree:
            tree = left_subtree
        else:
            tree[decision_node].extend([left_subtree, right_subtree])

        return tree
    
    def _tree_error_rate_info(self, tree, error_rates_list):
        if self._is_leaf_node(tree):
            *_, leaf_error_rate = tree.split()
            error_rates_list.append(np.float128(leaf_error_rate))
        else:
            decision_node = next(iter(tree))
            left_subtree, right_subtree = tree[decision_node]
            self._tree_error_rate_info(left_subtree, error_rates_list)
            self._thre_error_rate_info(right_subtree, error_rates_list)

        RT = sum(error_rates_list)
        num_leaf_nodes = len(error_rates_list)

        return RT, num_leaf_nodes
    
    @staticmethod
    def _cppp_alpha_eff(decision_node_Rt, leaf_nodes_RTt, num_leafs):
        return (decision_node_Rt - leaf_nodes_RTt) / (num_leafs - 1)
    
    def _find_weakest_nodes(self, tree, weakest_node_info):
        if self._is_leaf_node(tree):
            return tree
        
        decision_node = next(iter(tree))
        left_subtree, right_subtree = tree[decision_node]
        *_, decision_node_error_rate = decision_node.split()

        Rt = np.float128(decision_node_error_rate)
        RTt, num_leaf_nodes = self._tree_error_rate_info(tree, [])
        cpp_alpha = self._cppp_alpha_eff(Rt, RTt, num_leaf_nodes)
        decision_node_index, min_cpp_alpha_index = 0, 1

        if cpp_alpha <= weakest_node_info[min_cpp_alpha_index]:
            weakest_node_info[decision_node_index] = decision_node
            weakest_node_info[min_cpp_alpha_index] = cpp_alpha

        self._find_weakest_nodes(left_subtree, weakest_node_info)
        self._find_weakest_nodes(right_subtree, weakest_node_info)

        return weakest_node_info
    
    def _prune_tree(self, tree, weakest_node):
        if self._is_leaf_node(tree):
            return tree
        
        decision_node = next(iter(tree))
        left_subtree, right_subtree = tree[decision_node]
        left_subtree_index, right_subtree_index = 0, 1
        _, leaf_node = weakest_node.split('as_leaf')

        if weakest_node is decision_node:
            tree = weakest_node
        if weakest_node in left_subtree:
            tree[decision_node][left_subtree_index] = leaf_node
        if weakest_node in right_subtree:
            tree[decision_node][right_subtree_index] = leaf_node

        self._prune_tree(left_subtree, weakest_node)
        self._prune_tree(right_subtree, weakest_node)

        return tree
    
    def cost_complexity_pruning_path(self, X: pd.DataFrame, y: pd.Series):
        tree = self._grow_tree(X, y)
        tree_error_rate, _ = self._tree_error_rate_info(tree, [])
        error_rates = [tree_error_rate]
        cpp_alpha_list = [0.0]

        while not self._is_leaf_node(tree):
            initial_node = [None, np.inf]
            weakest_node, cpp_alpha = self._find_weakest_nodes(tree, initial_node)
            tree = self._prune_tree(tree, weakest_node)
            tree_error_rate, _ = self._tree_error_rate_info(tree, [])

            error_rates.append(tree_error_rate)
            cpp_alpha_list.append(cpp_alpha)

        return np.array(cpp_alpha_list), np.array(error_rates)
    
    def _cpp_tree_error_rate(self, tree_error_rate, num_leaf_nodes):
        return tree_error_rate + self.ccp_alpha*num_leaf_nodes
    
    def _optinat_tree(self, X, y):
        tree = self._grow_tree(X, y)
        min_RT_alpha, final_tree = np.inf, None

        while not self._is_leaf_node(tree):
            RT, num_leaf_nodes = self._tree_error_rate_info(tree, [])
            current_RT_aplha = self._cpp_tree_error_rate(RT, num_leaf_nodes)

            if current_RT_aplha <= min_RT_alpha:
                min_RT_alpha = current_RT_aplha
                final_tree = deepcopy(tree)

            initial_node = [None, np.inf]
            weakest_node, _ = self._find_weakest_nodes(tree, initial_node)
            tree = self._prune_tree(tree, weakest_node)

        return final_tree
    
    def fit(self, X: pd.DataFrame, y: pd.Series):
        self.tree = self._optimal_tree(X, y)

    def _traverse_tree(self, sample, tree):
        if self._is_leaf_node(tree):
            leaf, _ = tree.split()
            return leaf
        
        decision_node = next(iter(tree))
        left_node, right_node = tree[decision_node]
        feature, other = decision_node.split(' <=')
        threshold, *_ = other.split()
        feature_value = sample[feature]

        if np.float128(feature_value) <= np.float128(threshold):
            next_node = self._traverse_tree(sample, left_node)
        else:
            next_node = self._traverse_tree(sample, right_node)

    def predict(self, samples: pd.DataFrame):
        results = samples.apply(self._traverse_tree, args=(self.tree,), axis=1)

        return np.array(results.astype(self._y_dtype))