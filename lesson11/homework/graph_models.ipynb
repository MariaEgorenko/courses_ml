{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8240fb0c",
   "metadata": {},
   "source": [
    "### Основные типы графовых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9566a0",
   "metadata": {},
   "source": [
    "#### Графовые нейронные сети (GNN — Graph Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9668b",
   "metadata": {},
   "source": [
    "Это общий класс моделей, способных обрабатывать графы. Основная идея — агрегация информации от соседей для обновления представления узла.\n",
    "\n",
    "Цель GNN — научиться представлять каждый узел (или весь граф) в виде вектора (эмбеддинга), который учитывает:\n",
    "- признаки самого узла;\n",
    "- признаки его соседей;\n",
    "- структуру графа.\n",
    "\n",
    "Эти эмбеддинги затем используются для задач:\n",
    "- классификации узлов (например, определить, является ли пользователь ботом);\n",
    "- классификации графов (например, токсична ли молекула);\n",
    "- предсказания рёбер (например, предсказать, подружатся ли два пользователя).\n",
    "\n",
    "**Основная идея: Message Passing (передача сообщений)**\n",
    "\n",
    "Каждый узел обменивается информацией со своими соседями. Этот процесс повторяется несколько раз (слоёв), и на каждом шаге узел обновляет своё представление.\n",
    "\n",
    "Алгоритм одного слоя GNN:\n",
    "1. Агрегация: собрать представления соседей;\n",
    "2. Обновление: обновить своё представление на основе собственного и агрегированного от соседей.\n",
    "Формально:\n",
    "$$\\mathbf{h}_v^{(k)} = \\sigma \\left( \\mathbf{W}^{(k)} \\cdot \\text{AGGREGATE}^{(k)} \\left( \\left\\{ \\mathbf{h}_u^{(k-1)} \\mid u \\in \\mathcal{N}(v) \\right\\} \\cup \\left\\{ \\mathbf{h}_v^{(k-1)} \\right\\} \\right) \\right)$$\n",
    "где:\n",
    "- $\\mathbf{h}_v^{(k)}$ - эмбеддинг узла *v* на слое *k*;\n",
    "- $\\mathcal{N}(v)$ - соседи узла *v*;\n",
    "- $\\text{AGGREGATE}$ - функция агрегации (среднее, сумма, максимум и т.д.);\n",
    "- $\\mathbf{W}^{(k)}$ - обучаемая матрица весов;\n",
    "- $\\sigma$ - нелинейная функция активации (например, ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238e978",
   "metadata": {},
   "source": [
    "**Пример: как работает GNN (визуализация в тексте)**\n",
    "\n",
    "Рассмотрим простой граф из 4х узлов:\n",
    "\n",
    "<img src=\"src/graph_1.svg\" width=\"300\" height=\"200\">\n",
    "\n",
    "- Узлы: A, B, C, D\n",
    "- Рёбра: A–B, A–C, B–D, C–D\n",
    "\n",
    "Допустим, у каждого узла есть начальный признак:\n",
    "- A: [1, 0]\n",
    "- B: [0, 1]\n",
    "- C: [1, 1]\n",
    "- D: [0, 0]\n",
    "\n",
    "1. Шаг 0 (начальные эмбеддинги)\n",
    "2. Шаг 1 (первый слой GNN — агрегация соседей через среднее):\n",
    "   \n",
    "   - Для A: соседи = B, C → среднее = ([0,1] + [1,1]) / 2 = [0.5, 1.0]\n",
    "  \n",
    "        Новое представление A = MLP([1,0] + [0.5,1.0]) → например, [0.8, 0.9]\n",
    "   - Для D: соседи = B, C → среднее = ([0,1] + [1,1]) / 2 = [0.5, 1.0]\n",
    "        \n",
    "        Новое D = MLP([0,0] + [0.5,1.0]) → например, [0.4, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e2f48",
   "metadata": {},
   "source": [
    "Где применяются GNN:\n",
    "- Химия: Предсказание свойств молекул (узлы = атомы, рёбра = связи);\n",
    "- Соцсети: Обнаружение сообществ, рекомендации;\n",
    "- Биоинформатика: Протеин-протеин взаимодействия;\n",
    "- Рекомендации: Пользователь–товар граф\n",
    "- Финансы: Обнаружение мошенничества в транзакциях"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b9426",
   "metadata": {},
   "source": [
    "#### Графовые автокодировщики (Graph Autoencoders, GAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a5cf0",
   "metadata": {},
   "source": [
    "Графовый автокодировщик (GAE) — это нейросетевая модель, которая обучается без учителя (unsupervised) для получения низкоразмерных векторных представлений (эмбеддингов) узлов графа, сохраняя при этом его структуру.\n",
    "\n",
    "**Цель**: Научиться кодировать граф так, чтобы по эмбеддингам можно было восстановить связи (рёбра) между узлами.\n",
    "\n",
    "Это особенно полезно, когда:\n",
    "- Нет меток у узлов (например, в соцсетях);\n",
    "- Нужно предсказать, появится ли связь между двумя узлами (link prediction);\n",
    "- Требуется сжатое представление графа для последующих задач (кластеризация, визуализация и т.д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a759308",
   "metadata": {},
   "source": [
    "**Архитектура GAE**\n",
    "\n",
    "GAE состоит из двух частей:\n",
    "1. **Энкодер (Encoder)**\n",
    "\n",
    "Преобразует каждый узел $v_i$ в скрытое представление (эмбеддинг) $z_i \\in \\mathbb{R}^d, \\text{ где } d \\ll N$ (N — число узлов).\n",
    "\n",
    "Чаще всего энкодер — это GNN (например, GCN):\n",
    "$$Z=GCN(X,A)$$\n",
    "- *X* - матрица признаков узлов (если нет признаков, можно использовать единичную матрицу *I*)\n",
    "- *A* - матрица смежности графа\n",
    "- *Z* - матрица эмбеддингов размером $N×d$\n",
    "\n",
    "2. **Декодер (Decoder)**\n",
    "\n",
    "Восстанавливает (предсказывает) матрицу смежности из $\\hat{A}$ эмбеддингов Z.\n",
    "\n",
    "Самый простой декодер — внутреннее произведение:\n",
    "$$\\hat{A}_{ij} = \\sigma(z_i^{T}  z_j)$$\n",
    "где $\\sigma$ - сигмоида (дает вероятность существования ребра между *i* и *j*).\n",
    "\n",
    "**Функция потерь:**\n",
    "\n",
    "Обычно используется бинарная кросс-энтропия между истинной матрицей смежности *A* и предсказанной $\\hat{A}$:\n",
    "$$\\mathcal{L} = - \\sum_{i,j} \\left[ A_{ij} \\log \\hat{A}_{ij} + (1 - A_{ij}) \\log(1 - \\hat{A}_{ij}) \\right]$$\n",
    "На практике часто применяют взвешенную или сэмплированную версию, чтобы сбалансировать редкие рёбра (графы обычно разрежены)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c29b7",
   "metadata": {},
   "source": [
    "**Пример: GAE на простом графе:**\n",
    "\n",
    "Рассмотрим маленький неориентированный граф из 4x узлов:\n",
    "\n",
    "- Узлы: A, B, C, D\n",
    "- Рёбра: A–B, B–C, C–D\n",
    "\n",
    "<img src=\"src/graph_2.svg\" width=\"200\" height=\"50\">\n",
    "\n",
    "**Шаг 1: Исходные данные**\n",
    "- Матрица смежности A :\n",
    "```text\n",
    "        [[0, 1, 0, 0],\n",
    "         [1, 0, 1, 0],\n",
    "         [0, 1, 0, 1],\n",
    "         [0, 0, 1, 0]]\n",
    "```\n",
    "- Признаки узлов $X = I_4$ (единичная матрица, т.к. признаков нет).\n",
    "\n",
    "**Шаг 2: Энкодер (GCN)**\n",
    "\n",
    "Пусть GCN даёт эмбеддинги размерности 2:\n",
    "\n",
    "| Узел | Эмбеддинг $Z_1$ |\n",
    "|---|---|\n",
    "|A|[0.8, -0.3]|\n",
    "|B|[0.6, 0.1]|\n",
    "|C|[-0.5, 0.7]|\n",
    "|D|[-0.9, 0.4]|\n",
    "\n",
    "**Шаг 3: Декодер (внутреннее произведение + сигмоида)**\n",
    "\n",
    "Вычислим $\\hat{A}_{AB} = \\sigma([0.8, -0.3]·[0.6, 0.1]) = \\sigma(0.48 - 0.3) = \\sigma(0.45) ≈ 0.61$\n",
    "\n",
    "Аналогично для всех пар → получаем $\\hat{A}$, близкую к исходной $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc044e",
   "metadata": {},
   "source": [
    "**Применение GAE**\n",
    "- Link Prediction → Предсказать, появится ли дружба в соцсети или взаимодействие белков;\n",
    "- Node Clustering → Кластеризовать узлы по эмбеддингам (например, найти сообщества в Reddit);\n",
    "- Graph Visualization → Снизить размерность до 2D/3D для визуализации (аналог t-SNE, но с учётом структуры);\n",
    "- Предобучение для других задач → Эмбеддинги из GAE можно использовать как вход для классификатора (например, определить мошенника в графе транзакций)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ada5f",
   "metadata": {},
   "source": [
    "#### Графовые трансформеры (Graph Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a27bfb",
   "metadata": {},
   "source": [
    "Graph Transformers (GT) — это адаптация классических трансформеров (известных по NLP и Vision) к неевклидовым данным, то есть к графам. В отличие от CNN или RNN, трансформеры изначально не учитывают структуру графа — они работают с последовательностями и используют self-attention, которая по умолчанию полносвязна (каждый узел \"видит\" все остальные).\n",
    "\n",
    "Но в графах:\n",
    "- Не все узлы связаны;\n",
    "- Важна топология (расстояния, пути, сообщества);\n",
    "- Информация должна распространяться локально, как в GNN.\n",
    "\n",
    "Поэтому Graph Transformers модифицируют механизм внимания, чтобы он учитывал графовую структуру."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665b60d",
   "metadata": {},
   "source": [
    "**Основная идея**\n",
    "\n",
    "В классическом трансформере для последовательности токенов $x_1, ..., x_n$:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d}} \\right) V$$\n",
    "\n",
    "В Graph Transformer мы хотим, чтобы внимание не было полносвязным, а зависело от:\n",
    "- Существования ребра между узлами;\n",
    "- Расстояния в графе;\n",
    "- Позиционной информации (например, кратчайшего пути);\n",
    "- Типа узла/ребра (в гетерогенных графах).\n",
    "\n",
    "**Ключевые компоненты Graph Transformers**\n",
    "1. **Позиционные эмбеддинги (Positional Encodings)**\n",
    "\n",
    "Так как графы не имеют естественного порядка (в отличие от текста), нужно явно закодировать геометрию графа.\n",
    "\n",
    "Популярные подходы:\n",
    "- Laplacian Eigenvectors (спектральные координаты);\n",
    "- Shortest-path distances → эмбеддинги расстояний;\n",
    "- Random Walk Encodings;\n",
    "- Node2Vec / DeepWalk как предобученные признаки\n",
    "\n",
    "2. **Структурированное внимание**\n",
    "\n",
    "Вместо полносвязного внимания:\n",
    "- Ограничить внимание соседями (как в GNN) → Sparse Graph Transformer;\n",
    "- Взвешивать внимание по расстоянию → ближние узлы важнее;\n",
    "- Использовать edge features в attention\n",
    "\n",
    "Пример формулы с учётом рёбер:\n",
    "$$\\text{score}_{ij} = \\frac{(W_Q h_i)^T (W_K h_j)}{\\sqrt{d}} + \\psi(e_{ij}) + \\phi(\\text{dist}(i, j))$$\n",
    "где:\n",
    "- $h_i$ - признак узла *i*;\n",
    "- $e_{ij}$ - признак ребра между *i* и *j*;\n",
    "- $\\psi, \\phi$ - нейросети для кодирования ребер и расстояний.\n",
    "\n",
    "3. **Многоуровневая агрегация**\n",
    "\n",
    "Некоторые модели (например, GraphGPS) комбинируют:\n",
    "- Message Passing (GNN-стиль) — для локального контекста;\n",
    "- Global Attention (Transformer-стиль) — для долгосрочных зависимостей\n",
    "\n",
    "Это даёт лучшую выразительность и устойчивость к oversmoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcef5fe",
   "metadata": {},
   "source": [
    "**Примеры известных Graph Transformer моделей**\n",
    "\n",
    "|Модель|Год | Ключевая идея |\n",
    "|---|---|---|\n",
    "|Graphormer|2021 (Microsoft)|Использует расстояния, степени узлов и edge features в attention bias|\n",
    "|SAN (Structure-Aware Transformer)|2022|Встраивает структуру через Laplacian PE|\n",
    "|GraphGPS|2022|Гибрид GNN + Transformer|\n",
    "|GTN (Graph Transformer Network)|2020|Учитывает метапути в гетерогенных графах|\n",
    "|Phyloformer|2022|Для филогенетических деревьев (специфический граф)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebf9d3",
   "metadata": {},
   "source": [
    "**Пример: как работает Graphormer на маленьком графе**\n",
    "\n",
    "Рассмотрим простой граф из 4 узлов:\n",
    "<img src=\"src/graph_1.svg\" width=\"300\" height=\"200\">\n",
    "\n",
    "- Узлы: A, B, C, D\n",
    "- Рёбра: A–B, A–C, B–D, C–D\n",
    "\n",
    "**Шаг 1: Вычисляем матрицу расстояний:**\n",
    "\n",
    "|  |A|B|C|D|\n",
    "|---|---|---|---|---|\n",
    "|A|0|1|1|2|\n",
    "|B|1|0|2|1|\n",
    "|C|1|2|0|1|\n",
    "|D|2|1|1|0|\n",
    "\n",
    "**Шаг 2: Для каждой пары (*i*,*j*) добавляем bias в attention:**\n",
    "- Если dist(i,j) = 1 → сильный положительный bias;\n",
    "- Если dist(i,j) = 2 → слабый или отрицательный\n",
    "- Если нет пути → очень низкий вес\n",
    "\n",
    "**Шаг 3: Self-attention теперь \"знает\", что A и D не связаны напрямую, но связаны через B/C.**\n",
    "\n",
    "В итоге эмбеддинг узла A будет учитывать не только B и C, но и косвенно D — но с меньшим весом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21963ef4",
   "metadata": {},
   "source": [
    "**Визуализация: как меняется внимание**\n",
    "\n",
    "Допустим, у нас есть attention-матрица до и после добавления структурного bias:\n",
    "\n",
    "Без структуры (обычный Transformer):\n",
    "```text\n",
    "      A    B    C    D\n",
    "A  [0.25 0.25 0.25 0.25]\n",
    "B  [0.25 0.25 0.25 0.25]\n",
    "C  [0.25 0.25 0.25 0.25]\n",
    "D  [0.25 0.25 0.25 0.25]\n",
    "```\n",
    "→ Все узлы равнозначны.\n",
    "\n",
    "**С Graph Transformer (например, Graphormer):**\n",
    "```text\n",
    "      A    B    C    D\n",
    "A  [0.50 0.25 0.25 0.00]   ← A \"видит\" только B и C\n",
    "B  [0.30 0.40 0.00 0.30]   ← B видит A и D\n",
    "C  [0.30 0.00 0.40 0.30]   ← C видит A и D\n",
    "D  [0.00 0.35 0.35 0.30]   ← D видит B и C\n",
    "```\n",
    "→ Внимание соответствует структуре графа."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
